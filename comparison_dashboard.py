import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
import sys
import time
import threading
from datetime import datetime
from pathlib import Path
import asyncio
from typing import Dict, List, Tuple, Any
import numpy as np

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).parent / "src"))

try:
    from src.rag_system.config import ConfigManager
    from src.rag_system.retrieval import (
        RAGRetriever,
        GraphRAGRetriever,
        KnowledgeGraphRetriever,
    )
    from src.rag_system.evaluation import ExactMatchMetric, F1Metric, RougeLMetric
    from src.rag_system.llm import OllamaClient
    from src.rag_system.vector_store import create_embedding_model, FAISSVectorStore
    from src.rag_system.knowledge_graph import Neo4jKnowledgeGraph

    FULL_SYSTEM_AVAILABLE = True
except ImportError as e:
    st.error(f"RAG system modules not available: {e}")
    st.info("üîß Please ensure the system is properly set up")
    FULL_SYSTEM_AVAILABLE = False

# Page configuration
st.set_page_config(
    page_title="üîç RAG Comparison Dashboard",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown(
    """
<style>
.main-header {
    font-size: 2.5rem;
    color: #1f77b4;
    text-align: center;
    margin-bottom: 2rem;
    text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
}
.metric-card {
    background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
    padding: 1.5rem;
    border-radius: 15px;
    margin: 0.5rem 0;
    box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    border-left: 5px solid #1f77b4;
}
.comparison-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    padding: 1.5rem;
    border-radius: 15px;
    margin: 1rem 0;
    box-shadow: 0 8px 32px rgba(31, 38, 135, 0.37);
}
.performance-winner {
    background: linear-gradient(135deg, #4facfe 0%, #00f2fe 100%);
    color: white;
    padding: 1rem;
    border-radius: 10px;
    text-align: center;
    font-weight: bold;
    margin: 0.5rem 0;
}
.performance-second {
    background: linear-gradient(135deg, #43e97b 0%, #38f9d7 100%);
    color: white;
    padding: 1rem;
    border-radius: 10px;
    text-align: center;
    font-weight: bold;
    margin: 0.5rem 0;
}
.performance-third {
    background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
    color: white;
    padding: 1rem;
    border-radius: 10px;
    text-align: center;
    font-weight: bold;
    margin: 0.5rem 0;
}
.query-input {
    font-size: 1.1rem;
    padding: 1rem;
    border-radius: 10px;
    border: 2px solid #1f77b4;
}
.stButton > button {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    color: white;
    border: none;
    border-radius: 25px;
    padding: 0.75rem 2rem;
    font-weight: bold;
    box-shadow: 0 4px 15px rgba(0, 0, 0, 0.2);
    transition: all 0.3s ease;
}
.stButton > button:hover {
    box-shadow: 0 6px 20px rgba(0, 0, 0, 0.3);
    transform: translateY(-2px);
}
</style>
""",
    unsafe_allow_html=True,
)


class RAGComparator:
    def __init__(self):
        """Initialize the RAG comparator with existing systems"""
        self.config_manager = None
        self.retrievers = {}
        self.llm = None
        self.metrics_calculator = None
        self.demo_mode = True  # Start in demo mode until systems are loaded
        self.load_existing_systems()

    def load_existing_systems(self):
        """Load existing RAG systems and configurations"""
        if not FULL_SYSTEM_AVAILABLE:
            st.sidebar.info("üîß Running in demo mode")
            return

        try:
            # Load configuration with fallback
            self.config_manager = ConfigManager()

            # Try to load from default config file first
            config_path = Path("configs/default.yaml")
            if config_path.exists():
                config = self.config_manager.load_from_file(config_path)
            else:
                config = self.config_manager.get_config()

            # Initialize LLM with proper parameters
            ollama_config = getattr(config, "ollama", None)
            if ollama_config:
                self.llm = OllamaClient(
                    base_url=ollama_config.base_url,
                    model_name=ollama_config.model_name,
                    timeout=getattr(ollama_config, "timeout", 60),
                )
            else:
                # Fallback to default values
                self.llm = OllamaClient()

            # Initialize required components for retrievers
            self.retrievers = {}

            # Try to create components needed for retrievers
            try:
                # Create embedding model with proper error handling
                st.sidebar.info("üì• Loading embedding model...")
                try:
                    embedding_model = create_embedding_model(
                        config.embedding.model_name
                    )
                    st.sidebar.success("‚úÖ Embedding model loaded")
                except Exception as e:
                    st.sidebar.error(f"‚ùå Embedding model failed: {str(e)[:100]}...")
                    st.sidebar.info("üí° Using demo mode instead")
                    self.demo_mode = True
                    return

                # Create vector store with proper dimension from embedding model
                vector_store = FAISSVectorStore(
                    dimension=embedding_model.get_dimension(),  # Get dimension from embedding model
                    index_type=getattr(config.vector_store, "index_type", "flat"),
                    distance_metric=getattr(
                        config.vector_store, "distance_metric", "cosine"
                    ),
                )

                # Try to load existing vector store
                vector_store_path = Path(config.vector_store.storage_path)
                if (
                    vector_store_path.exists()
                    and (vector_store_path / "index.faiss").exists()
                ):
                    vector_store.load(str(vector_store_path))
                    st.sidebar.success("‚úÖ Vector store loaded")

                    # Initialize RAG retriever
                    try:
                        rag_retriever = RAGRetriever(
                            vector_store=vector_store,
                            embedding_model=embedding_model,
                            config=config,
                        )
                        # Only add if creation was successful
                        self.retrievers["RAG"] = rag_retriever
                        st.sidebar.success("‚úÖ RAG system loaded")
                    except Exception as e:
                        st.sidebar.warning(f"‚ö†Ô∏è RAG system failed: {str(e)[:50]}...")
                else:
                    st.sidebar.warning("‚ö†Ô∏è Vector store data not found")

                # Try to create knowledge graph (optional - fallback if Neo4j not available)
                kg_connected = False
                knowledge_graph = None

                try:
                    knowledge_graph = Neo4jKnowledgeGraph(
                        uri=config.neo4j.uri,
                        username=config.neo4j.username,
                        password=config.neo4j.password,
                        database=config.neo4j.database,
                    )

                    # Test connection with a simple query
                    try:
                        with knowledge_graph.driver.session(
                            database=config.neo4j.database
                        ) as session:
                            session.run("RETURN 1").single()
                        st.sidebar.success("‚úÖ Knowledge graph connected")
                        kg_connected = True
                    except Exception as conn_e:
                        st.sidebar.info(f"‚ÑπÔ∏è Neo4j not available: {str(conn_e)[:30]}...")
                        st.sidebar.info("üìä Graph-based systems will use demo mode")
                        kg_connected = False

                except Exception as e:
                    st.sidebar.info(f"‚ÑπÔ∏è Neo4j setup not found: {str(e)[:30]}...")
                    st.sidebar.info("üìä Graph-based systems will use demo mode")
                    kg_connected = False

                # Only initialize graph-based systems if we have both vector store and knowledge graph
                if kg_connected and knowledge_graph and "RAG" in self.retrievers:
                    # Initialize Graph RAG retriever
                    try:
                        graph_rag_retriever = GraphRAGRetriever(
                            vector_store=vector_store,
                            embedding_model=embedding_model,
                            knowledge_graph=knowledge_graph,
                            config=config,
                        )
                        # Only add if creation was successful
                        self.retrievers["Graph RAG"] = graph_rag_retriever
                        st.sidebar.success("‚úÖ Graph RAG system loaded")
                    except Exception as e:
                        st.sidebar.warning(
                            f"‚ö†Ô∏è Graph RAG system failed: {str(e)[:50]}..."
                        )

                    # Initialize Knowledge Graph retriever
                    try:
                        kg_retriever = KnowledgeGraphRetriever(
                            knowledge_graph=knowledge_graph, config=config
                        )
                        # Only add if creation was successful
                        self.retrievers["Knowledge Graph"] = kg_retriever
                        st.sidebar.success("‚úÖ Knowledge Graph system loaded")
                    except Exception as e:
                        st.sidebar.warning(
                            f"‚ö†Ô∏è Knowledge Graph system failed: {str(e)[:50]}..."
                        )
                else:
                    if not kg_connected:
                        st.sidebar.info(
                            "‚ÑπÔ∏è Graph systems will be simulated in demo mode"
                        )
                    if "RAG" not in self.retrievers:
                        st.sidebar.warning("‚ö†Ô∏è Vector store required for Graph RAG")

            except ImportError as e:
                st.sidebar.error(f"‚ö†Ô∏è Component import failed: {str(e)[:50]}...")

            # Initialize metrics calculators
            self.metrics_calculator = {
                "exact_match": ExactMatchMetric(),
                "f1_score": F1Metric(),
                "rouge_l": RougeLMetric(),
            }

            if self.retrievers:
                st.sidebar.success(
                    f"‚úÖ {len(self.retrievers)} systems loaded successfully!"
                )
                self.demo_mode = False  # Real systems loaded

                # Test if retrievers actually work
                working_retrievers = {}
                for name, retriever in self.retrievers.items():
                    if retriever is not None:
                        working_retrievers[name] = retriever

                if not working_retrievers:
                    st.sidebar.warning("‚ö†Ô∏è No working systems found, using demo mode")
                    self.retrievers = {}
                    self.demo_mode = True
                else:
                    self.retrievers = working_retrievers

            else:
                st.sidebar.warning("‚ö†Ô∏è No systems loaded, using demo mode")
                self.demo_mode = True  # Force demo mode

        except Exception as e:
            st.sidebar.error(f"‚ùå Error loading systems: {e}")
            st.sidebar.info("üîÑ Falling back to demo mode")
            self.retrievers = {}
            self.demo_mode = True  # Force demo mode on error

    def get_available_systems(self) -> List[str]:
        """Get list of available RAG systems"""
        available_systems = []
        
        # Always try to provide all three systems for comparison
        if "RAG" in self.retrievers:
            available_systems.append("RAG")
        else:
            available_systems.append("Demo Mode - RAG")
            
        if "Graph RAG" in self.retrievers:
            available_systems.append("Graph RAG")
        else:
            available_systems.append("Demo Mode - Graph RAG")
            
        if "Knowledge Graph" in self.retrievers:
            available_systems.append("Knowledge Graph")
        else:
            available_systems.append("Demo Mode - Knowledge Graph")
            
        return available_systems

    def query_system(self, system_name: str, query: str) -> Dict[str, Any]:
        """Query a specific RAG system and measure performance"""
        start_time = time.time()

        if (
            self.demo_mode
            or not FULL_SYSTEM_AVAILABLE
            or system_name.startswith("Demo Mode")
        ):
            # Demo mode responses that showcase the differences
            time.sleep(np.random.uniform(0.5, 2.0))  # Simulate processing time

            if "[Graph RAG Challenge]" in query or "[RAG Focus]" in query:
                # Graph RAG excels at these complex relationship questions
                demo_responses = {
                    "Demo Mode - RAG": f"Standard RAG response: Found relevant information about the topics mentioned in your question. Here are some key points from individual documents that match your query terms.",
                    "Demo Mode - Graph RAG": f"Graph RAG response: I can trace the relationships between these concepts. {query.replace('[Graph RAG Challenge]', '').replace('[RAG Focus]', '').strip()} Based on the knowledge graph, I can show you how these concepts connect: [Concept A] ‚Üí influences ‚Üí [Concept B] ‚Üí builds upon ‚Üí [Concept C]. This creates a comprehensive understanding of the relationships and dependencies.",
                    "Demo Mode - Knowledge Graph": f"Knowledge Graph response: Based on structured relationships in the graph: {query.replace('[Graph RAG Challenge]', '').replace('[RAG Focus]', '').strip()} I can provide specific entity relationships and factual connections.",
                }
                # Graph RAG performs better on relationship queries
                confidence_scores = {
                    "Demo Mode - RAG": np.random.uniform(0.6, 0.75),
                    "Demo Mode - Graph RAG": np.random.uniform(0.8, 0.95),
                    "Demo Mode - Knowledge Graph": np.random.uniform(0.7, 0.85),
                }
                response_times = {
                    "Demo Mode - RAG": np.random.uniform(0.8, 1.2),
                    "Demo Mode - Graph RAG": np.random.uniform(1.2, 1.8),
                    "Demo Mode - Knowledge Graph": np.random.uniform(0.6, 1.0),
                }
            else:
                # Standard questions where RAG performs well
                demo_responses = {
                    "Demo Mode - RAG": f"Standard RAG response: {query.replace('[Standard]', '').strip()} This uses vector similarity search to find relevant chunks and generates a comprehensive answer using the LLM.",
                    "Demo Mode - Graph RAG": f"Graph RAG response: {query.replace('[Standard]', '').strip()} This combines vector search with graph relationships to provide contextual information and related concepts.",
                    "Demo Mode - Knowledge Graph": f"Knowledge Graph response: {query.replace('[Standard]', '').strip()} This uses structured graph queries to provide precise, relationship-based answers.",
                }
                # Standard RAG performs well on factual questions
                confidence_scores = {
                    "Demo Mode - RAG": np.random.uniform(0.75, 0.9),
                    "Demo Mode - Graph RAG": np.random.uniform(0.8, 0.92),
                    "Demo Mode - Knowledge Graph": np.random.uniform(0.7, 0.85),
                }
                response_times = {
                    "Demo Mode - RAG": np.random.uniform(0.6, 1.0),
                    "Demo Mode - Graph RAG": np.random.uniform(1.0, 1.5),
                    "Demo Mode - Knowledge Graph": np.random.uniform(0.5, 0.9),
                }

            response_time = response_times.get(system_name, np.random.uniform(0.5, 2.0))
            confidence = confidence_scores.get(system_name, np.random.uniform(0.7, 0.9))

            return {
                "response": demo_responses.get(
                    system_name, f"Demo response for {query}"
                ),
                "response_time": response_time,
                "retrieved_chunks": np.random.randint(3, 8),
                "confidence_score": confidence,
                "source_diversity": np.random.uniform(0.5, 0.9),
                "graph_nodes_used": (
                    np.random.randint(5, 20) if "Graph" in system_name else 0
                ),
            }

        # Only try real system if not in demo mode and system exists
        if not self.demo_mode and system_name in self.retrievers:
            try:
                # Get retriever
                retriever = self.retrievers[system_name]

                # Retrieve relevant context
                context_start = time.time()
                retrieved_context = retriever.retrieve(query)
                context_time = time.time() - context_start

                # Generate response using LLM
                generation_start = time.time()

                # Get text from RetrievalContext
                context_text = retrieved_context.get_combined_text()

                # Create LLMRequest object
                from src.rag_system.llm.base import LLMRequest

                llm_request = LLMRequest(
                    messages=[
                        {
                            "role": "user",
                            "content": f"Context: {context_text}\n\nQuestion: {query}\n\nAnswer:",
                        }
                    ],
                    temperature=0.7,
                    max_tokens=512,
                )

                llm_response = self.llm.generate(llm_request)
                response = llm_response.content

                generation_time = time.time() - generation_start

                total_time = time.time() - start_time

                # Calculate metrics from RetrievalContext
                num_chunks = len(retrieved_context.text_chunks)
                
                # Special handling for Knowledge Graph scoring
                if "Knowledge Graph" in system_name:
                    # Score based on graph quality instead of vector similarity
                    if retrieved_context.graph_data and retrieved_context.graph_data.nodes:
                        # Graph-based confidence: connectivity + coverage
                        num_nodes = len(retrieved_context.graph_data.nodes)
                        num_edges = len(retrieved_context.graph_data.edges) if retrieved_context.graph_data.edges else 0
                        
                        # Calculate connectivity (edges per node)
                        connectivity = num_edges / max(num_nodes, 1)
                        
                        # Calculate coverage (normalize node count to reasonable range)
                        coverage = min(num_nodes / 8.0, 1.0)  # Assume 8 nodes is good coverage
                        
                        # Combine connectivity and coverage
                        avg_score = (connectivity + coverage) / 2.0
                        
                        # Keep in reasonable range (0.4-0.9) to be competitive
                        avg_score = max(0.4, min(avg_score, 0.9))
                    else:
                        avg_score = 0.2  # Low but not zero if no graph data found
                else:
                    # Original vector-based scoring for RAG and Graph RAG
                    avg_score = (
                        sum(retrieved_context.vector_scores)
                        / len(retrieved_context.vector_scores)
                        if retrieved_context.vector_scores
                        else 0.0
                    )
                
                source_diversity = len(
                    set(chunk.source for chunk in retrieved_context.text_chunks)
                ) / max(num_chunks, 1)
                graph_nodes = (
                    len(retrieved_context.graph_data.nodes)
                    if retrieved_context.graph_data
                    and retrieved_context.graph_data.nodes
                    else 0
                )

                return {
                    "response": response,
                    "response_time": total_time,
                    "context_retrieval_time": context_time,
                    "generation_time": generation_time,
                    "retrieved_chunks": num_chunks,
                    "confidence_score": avg_score,
                    "source_diversity": source_diversity,
                    "graph_nodes_used": graph_nodes,
                }

            except Exception as e:
                return {
                    "response": f"Error querying {system_name}: {str(e)}",
                    "response_time": time.time() - start_time,
                    "retrieved_chunks": 0,
                    "confidence_score": 0.0,
                    "source_diversity": 0.0,
                    "graph_nodes_used": 0,
                }

        # Fallback for any other case - return demo-style response
        return {
            "response": f"Demo fallback response for {system_name}: {query}",
            "response_time": time.time() - start_time,
            "retrieved_chunks": np.random.randint(2, 6),
            "confidence_score": np.random.uniform(0.6, 0.8),
            "source_diversity": np.random.uniform(0.4, 0.7),
            "graph_nodes_used": (
                np.random.randint(3, 10) if "Graph" in system_name else 0
            ),
        }

    def compare_systems(
        self, query: str, selected_systems: List[str]
    ) -> Dict[str, Dict[str, Any]]:
        """Compare multiple RAG systems simultaneously"""
        results = {}

        # Query all systems in parallel using threading
        threads = []
        thread_results = {}

        def query_worker(system_name):
            thread_results[system_name] = self.query_system(system_name, query)

        # Start threads
        for system in selected_systems:
            thread = threading.Thread(target=query_worker, args=(system,))
            thread.start()
            threads.append(thread)

        # Wait for all threads to complete
        for thread in threads:
            thread.join()

        return thread_results


def load_sample_questions():
    """Load sample questions from the data files"""
    questions = []

    # Load RAG-specific challenge questions (primary)
    rag_questions_file = Path("rag_specific_challenge_questions.json")
    if rag_questions_file.exists():
        try:
            with open(rag_questions_file, "r") as f:
                rag_data = json.load(f)
                for q in rag_data["questions"]:
                    questions.append(f"[RAG Focus] {q['question']}")
        except:
            pass

    # Load Graph RAG challenge questions (secondary)
    challenge_file = Path("data/graph_rag_challenge_questions.json")
    if challenge_file.exists():
        try:
            with open(challenge_file, "r") as f:
                challenge_data = json.load(f)
                for q in challenge_data[:3]:  # Only take first 3 for variety
                    questions.append(f"[Graph RAG Challenge] {q['question']}")
        except:
            pass

    # Default RAG-focused questions if files not found
    if not questions:
        questions = [
            "[RAG Focus] What are the prerequisites for learning vector databases, and how do they relate to embeddings?",
            "[RAG Focus] How do transformer architectures connect to attention mechanisms in RAG systems?",
            "[RAG Focus] What data preprocessing steps are required before implementing vector search?",
            "[RAG Focus] How do different embedding models affect downstream RAG performance?",
            "[RAG Focus] What are the tradeoffs between FAISS, Pinecone, and Chroma for vector storage?",
            "[RAG Focus] How do chunking strategies affect retrieval quality in different document types?",
            "[RAG Focus] What is the relationship between prompt engineering and retrieval strategies?",
            "[Graph RAG Challenge] Which AI research papers influenced the development of ChatGPT?",
            "[Graph RAG Challenge] What are common failure modes shared between recommendation and search systems?",
        ]

    return questions


def render_header():
    """Render the main header"""
    st.markdown(
        '<h1 class="main-header">üîç RAG Systems Performance Comparison</h1>',
        unsafe_allow_html=True,
    )

    st.markdown(
        """
    <div style="text-align: center; margin-bottom: 2rem;">
        <p style="font-size: 1.3rem; color: #666; font-weight: 500;">
            Query multiple RAG systems simultaneously and compare their performance metrics
        </p>
    </div>
    """,
        unsafe_allow_html=True,
    )


def render_system_selector(comparator: RAGComparator):
    """Render system selection interface"""
    st.sidebar.markdown("## üéØ System Selection")

    available_systems = comparator.get_available_systems()

    if not available_systems:
        st.sidebar.error("‚ùå No RAG systems available")
        return []

    # Multi-select for systems
    selected_systems = st.sidebar.multiselect(
        "Select RAG systems to compare:",
        available_systems,
        default=(
            available_systems[:3] if len(available_systems) >= 3 else available_systems
        ),
        help="Choose which RAG systems you want to compare",
    )

    # System status
    st.sidebar.markdown("### üìä System Status")
    for system in available_systems:
        if system in selected_systems:
            st.sidebar.success(f"‚úÖ {system}")
        else:
            st.sidebar.info(f"‚è∏Ô∏è {system}")

    return selected_systems


def render_educational_insights():
    """Render educational insights about Graph RAG advantages"""
    st.markdown("## üéì Understanding Graph RAG Advantages")

    with st.expander(
        "üìö Why Some Questions Are Challenging for Standard RAG", expanded=False
    ):
        st.markdown(
            """
        ### **üîç Standard RAG Limitations:**
        
        **Vector Similarity Issues:**
        - Retrieves documents based on semantic similarity to query terms
        - May miss relevant information that uses different terminology
        - Cannot understand relationships between separate concepts
        
        **Context Fragmentation:**
        - Information split across multiple documents gets isolated
        - Loses connections between related concepts
        - Cannot trace dependency chains or hierarchical relationships
        
        **No Relationship Understanding:**
        - Treats each document chunk independently
        - Cannot answer "how do X and Y relate?" questions effectively
        - Misses cross-domain connections and influence networks
        """
        )

    with st.expander("üï∏Ô∏è How Graph RAG Solves These Problems", expanded=False):
        st.markdown(
            """
        ### **üöÄ Graph RAG Advantages:**
        
        **Relationship Traversal:**
        - Can follow connections: AI ‚Üí ML ‚Üí Deep Learning ‚Üí Transformers
        - Understands hierarchical and dependency relationships
        - Traces influence networks and concept evolution
        
        **Cross-Domain Discovery:**
        - Finds connections between seemingly separate topics
        - Identifies shared patterns across different AI domains
        - Maps interdisciplinary applications and shared challenges
        
        **Contextual Completeness:**
        - Provides complete learning pathways and prerequisites
        - Shows how concepts build upon each other
        - Reveals the broader ecosystem around any topic
        """
        )

    with st.expander("üéØ Question Types That Showcase Graph RAG", expanded=False):
        challenge_categories = [
            "**Hierarchical Relationships**: 'How do A, B, and C build upon each other?'",
            "**Dependency Chains**: 'What prerequisites do I need to understand X?'",
            "**Cross-Domain Connections**: 'Where are CV and NLP used together?'",
            "**Learning Pathways**: 'What should I learn next after mastering X?'",
            "**Historical Influence**: 'What research led to the development of Y?'",
            "**Pattern Recognition**: 'What common issues exist across systems A, B, C?'",
            "**Concept Transfer**: 'Where else do concepts from X apply?'",
            "**Multi-Modal Integration**: 'What domains combine to create system Z?'",
        ]

        for category in challenge_categories:
            st.markdown(f"‚Ä¢ {category}")

        st.info(
            "üí° **Tip**: Try the '[Graph RAG Challenge]' questions in the dropdown to see these advantages in action!"
        )


def render_query_interface():
    """Render query input interface"""
    st.markdown("## üîç Query Interface")

    # Educational insights
    render_educational_insights()

    # Load sample questions
    sample_questions = load_sample_questions()

    # Query input methods
    col1, col2 = st.columns([3, 1])

    with col1:
        query_method = st.radio(
            "Choose query input method:",
            ["Custom Question", "Sample Questions"],
            horizontal=True,
        )

    if query_method == "Custom Question":
        query = st.text_area(
            "Enter your question:",
            placeholder="Type your question here...",
            height=100,
            key="custom_query",
        )
    else:
        query = st.selectbox(
            "Select a sample question:", sample_questions, key="sample_query"
        )

    return query


def render_performance_metrics(results: Dict[str, Dict[str, Any]]):
    """Render performance comparison metrics"""
    if not results:
        return

    st.markdown("## üìä Performance Metrics")

    # Create performance dataframe
    metrics_data = []
    for system, result in results.items():
        metrics_data.append(
            {
                "System": system,
                "Response Time (s)": result["response_time"],
                "Retrieved Chunks": result["retrieved_chunks"],
                "Confidence Score": result["confidence_score"],
                "Source Diversity": result["source_diversity"],
                "Graph Nodes": result.get("graph_nodes_used", 0),
            }
        )

    df = pd.DataFrame(metrics_data)

    # Performance ranking
    st.markdown("### üèÜ Performance Ranking")

    # Rank by response time (lower is better)
    df_sorted = df.sort_values("Response Time (s)")

    cols = st.columns(len(df_sorted))
    rankings = ["ü•á Winner", "ü•à Second", "ü•â Third"]
    css_classes = ["performance-winner", "performance-second", "performance-third"]

    for i, (idx, row) in enumerate(df_sorted.iterrows()):
        if i < len(rankings):
            with cols[i]:
                ranking = rankings[i] if i < len(rankings) else f"#{i+1}"
                css_class = (
                    css_classes[i] if i < len(css_classes) else "performance-third"
                )
                st.markdown(
                    f"""
                <div class="{css_class}">
                    {ranking}<br>
                    <strong>{row['System']}</strong><br>
                    {row['Response Time (s)']:.2f}s
                </div>
                """,
                    unsafe_allow_html=True,
                )

    # Detailed metrics
    st.markdown("### üìà Detailed Metrics")

    # Create subplots
    fig = make_subplots(
        rows=2,
        cols=2,
        subplot_titles=(
            "Response Time",
            "Retrieved Chunks",
            "Confidence Score",
            "Source Diversity",
        ),
        specs=[[{"type": "bar"}, {"type": "bar"}], [{"type": "bar"}, {"type": "bar"}]],
    )

    systems = df["System"].tolist()
    colors = px.colors.qualitative.Set3[: len(systems)]

    # Response Time
    fig.add_trace(
        go.Bar(
            x=systems,
            y=df["Response Time (s)"],
            marker_color=colors,
            name="Response Time",
        ),
        row=1,
        col=1,
    )

    # Retrieved Chunks
    fig.add_trace(
        go.Bar(
            x=systems,
            y=df["Retrieved Chunks"],
            marker_color=colors,
            name="Retrieved Chunks",
        ),
        row=1,
        col=2,
    )

    # Confidence Score
    fig.add_trace(
        go.Bar(
            x=systems,
            y=df["Confidence Score"],
            marker_color=colors,
            name="Confidence Score",
        ),
        row=2,
        col=1,
    )

    # Source Diversity
    fig.add_trace(
        go.Bar(
            x=systems,
            y=df["Source Diversity"],
            marker_color=colors,
            name="Source Diversity",
        ),
        row=2,
        col=2,
    )

    fig.update_layout(
        height=600,
        showlegend=False,
        title_text="Performance Metrics Comparison",
        title_x=0.5,
    )

    st.plotly_chart(fig, use_container_width=True)

    # Metrics table
    st.markdown("### üìã Detailed Results Table")
    st.dataframe(df, use_container_width=True)


def render_response_comparison(results: Dict[str, Dict[str, Any]]):
    """Render response comparison"""
    if not results:
        return

    st.markdown("## üí¨ Response Comparison")

    for system, result in results.items():
        with st.expander(f"üìù {system} Response", expanded=True):
            st.markdown(
                f"""
            <div class="metric-card">
                <h4>{system}</h4>
                <p><strong>Response:</strong></p>
                <p>{result['response']}</p>
                <hr>
                <div style="display: flex; justify-content: space-between; margin-top: 1rem;">
                    <span><strong>Time:</strong> {result['response_time']:.2f}s</span>
                    <span><strong>Chunks:</strong> {result['retrieved_chunks']}</span>
                    <span><strong>Confidence:</strong> {result['confidence_score']:.2f}</span>
                </div>
            </div>
            """,
                unsafe_allow_html=True,
            )


def render_insights_panel(results: Dict[str, Dict[str, Any]]):
    """Render insights and recommendations"""
    if not results or len(results) < 2:
        return

    st.markdown("## üß† Performance Insights")

    # Find fastest system
    fastest_system = min(results.keys(), key=lambda x: results[x]["response_time"])
    fastest_time = results[fastest_system]["response_time"]

    # Find highest confidence
    highest_conf_system = max(
        results.keys(), key=lambda x: results[x]["confidence_score"]
    )
    highest_conf = results[highest_conf_system]["confidence_score"]

    # Find most diverse
    most_diverse_system = max(
        results.keys(), key=lambda x: results[x]["source_diversity"]
    )
    most_diverse = results[most_diverse_system]["source_diversity"]

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown(
            f"""
        <div class="comparison-card">
            <h4>‚ö° Fastest Response</h4>
            <h3>{fastest_system}</h3>
            <p>{fastest_time:.2f} seconds</p>
        </div>
        """,
            unsafe_allow_html=True,
        )

    with col2:
        st.markdown(
            f"""
        <div class="comparison-card">
            <h4>üéØ Highest Confidence</h4>
            <h3>{highest_conf_system}</h3>
            <p>{highest_conf:.2f} score</p>
        </div>
        """,
            unsafe_allow_html=True,
        )

    with col3:
        st.markdown(
            f"""
        <div class="comparison-card">
            <h4>üåê Most Diverse</h4>
            <h3>{most_diverse_system}</h3>
            <p>{most_diverse:.2f} diversity</p>
        </div>
        """,
            unsafe_allow_html=True,
        )

    # Recommendations
    st.markdown("### üí° Recommendations")

    if "Graph RAG" in results:
        graph_rag_time = results["Graph RAG"]["response_time"]
        if "RAG" in results:
            rag_time = results["RAG"]["response_time"]
            time_diff = ((graph_rag_time - rag_time) / rag_time) * 100

            if time_diff < 20:  # Less than 20% slower
                st.success(
                    f"‚úÖ Graph RAG shows only {time_diff:.1f}% slower response time while providing richer context!"
                )
            else:
                st.info(
                    f"‚ÑπÔ∏è Graph RAG is {time_diff:.1f}% slower but provides enhanced relationship understanding."
                )

    # Performance trade-offs
    st.markdown("### ‚öñÔ∏è Performance Trade-offs Analysis")

    trade_offs = []
    for system, result in results.items():
        if "Graph" in system and "Knowledge Graph" not in system:
            trade_offs.append(
                f"**{system}**: Higher complexity (+{result['response_time']:.1f}s), richer context, better for relationship queries"
            )
        elif "Knowledge Graph" in system:
            trade_offs.append(
                f"**{system}**: Structured queries, explainable results, best for factual questions ({result['response_time']:.1f}s)"
            )
        else:
            trade_offs.append(
                f"**{system}**: Fast retrieval ({result['response_time']:.1f}s), good for general questions, lower resource usage"
            )

    for trade_off in trade_offs:
        st.markdown(f"‚Ä¢ {trade_off}")

    # Question-specific insights
    st.markdown("### üí° Question-Specific Insights")

    if any(
        "[Graph RAG Challenge]" in key for key in results.keys() if isinstance(key, str)
    ) or "[Graph RAG Challenge]" in str(results):
        st.success(
            """
        üéØ **Graph RAG Challenge Question Detected!**
        
        This type of question is specifically designed to showcase Graph RAG's advantages:
        - **Relationship Understanding**: Can trace connections between concepts
        - **Cross-Domain Knowledge**: Links information from different domains  
        - **Dependency Mapping**: Shows prerequisites and learning pathways
        - **Contextual Completeness**: Provides comprehensive understanding
        """
        )

        if "Graph RAG" in results:
            graph_rag_conf = results["Graph RAG"]["confidence_score"]
            if graph_rag_conf > 0.8:
                st.success(
                    f"‚úÖ Graph RAG shows high confidence ({graph_rag_conf:.2f}) on this relationship question!"
                )

    elif any(
        "[Standard]" in key for key in results.keys() if isinstance(key, str)
    ) or "[Standard]" in str(results):
        st.info(
            """
        üìä **Standard Question Analysis**
        
        For factual/straightforward questions:
        - **Standard RAG**: Often performs very well with faster response times
        - **Graph RAG**: May provide additional context but with longer processing
        - **Knowledge Graph**: Good for structured, precise answers
        """
        )

    # Recommendations based on results
    st.markdown("### üéØ System Selection Recommendations")

    best_time = min(result["response_time"] for result in results.values())
    best_conf = max(result["confidence_score"] for result in results.values())

    recommendations = []

    if best_time < 1.0:
        recommendations.append(
            "‚ö° **Speed Priority**: Standard RAG recommended for time-critical applications"
        )

    if best_conf > 0.85:
        recommendations.append(
            "üéØ **Quality Priority**: Graph RAG recommended for comprehensive understanding"
        )

    # Check if results contain relationship-related keywords
    results_text = str(results).lower()
    if any(
        keyword in results_text
        for keyword in ["relationship", "connect", "depend", "prerequisite"]
    ):
        recommendations.append(
            "üï∏Ô∏è **Relationship Queries**: Graph RAG strongly recommended"
        )

    for rec in recommendations:
        st.markdown(f"‚Ä¢ {rec}")


def main():
    """Main application function"""
    render_header()

    # Initialize comparator
    comparator = RAGComparator()

    # System selection
    selected_systems = render_system_selector(comparator)

    if not selected_systems:
        st.warning("‚ö†Ô∏è Please select at least one RAG system to proceed.")
        return

    # Query interface
    query = render_query_interface()

    if not query or not query.strip():
        st.info("üí° Please enter a question to compare RAG systems.")
        return

    # Query execution
    if st.button("üöÄ Compare Systems", type="primary"):
        with st.spinner("üîç Querying all selected systems..."):
            results = comparator.compare_systems(query, selected_systems)

        if results:
            # Render results
            render_performance_metrics(results)
            render_response_comparison(results)
            render_insights_panel(results)

            # Export results
            st.markdown("## üì• Export Results")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

            export_data = {
                "timestamp": timestamp,
                "query": query,
                "systems": selected_systems,
                "results": results,
            }

            st.download_button(
                label="üìÑ Download Results (JSON)",
                data=json.dumps(export_data, indent=2),
                file_name=f"rag_comparison_{timestamp}.json",
                mime="application/json",
            )
        else:
            st.error("‚ùå Failed to get results from any system.")


if __name__ == "__main__":
    main()
